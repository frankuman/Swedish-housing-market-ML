{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oliver Bölin\n",
    "## Blekinge Tekniska Högskola\n",
    "\n",
    "### Run this step by step to CREATE the algorithm. It will name it vm.pkl after and then the vm.py file will use it in the frontend\n",
    "### This is so we dont have to relearn the algorithm everytime. You just have to do this step because\n",
    "### 1. Github doesnt like big files\n",
    "### Thats it. \n",
    "## Before you run this remember to UNZIP the prop.rar and name it prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. import\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from geopy.geocoders import Nominatim\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import geopy.geocoders\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from geopy.geocoders import Nominatim\n",
    "import requests\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.0\n",
    "## Data manipulation\n",
    "### Removal of unessecary parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example from the raw data: {\"street\": \"Atlasv\\u00e4gen 23\", \"property_type\": \"L\\u00e4genhet\", \"build_year\": 0, \"ownership_type\": \"Bostadsr\\u00e4tt\", \"housing_form\": \"APARTMENT\", \"living_area\": 57, \"land_area\": 0, \"county\": \"Nacka\", \"area\": \"Sickla\", \"price\": 2800000, \"wanted_price\": 2495000, \"latitude\": 59.304116967271526, \"longitude\": 18.124639596053395, \"fee\": 3186, \"operating_cost\": 0, \"rooms\": 2, \"floor\": \"\", \"balcony\": \"\", \n",
    "# \"construction_date\": 0, \"association\": \"\", \"broker\": \"Svensk Fastighetsf\\u00f6rmedling Nacka\", \"sold_at\": 1576800000, \n",
    "# \"url\": \"https://www.hemnet.se/salda/lagenhet-2rum-sickla-nacka-kommun-atlasvagen-23-1123249\", \"price_change\": \"+12\\u00a0%\", \"story\": \"\"}\n",
    "# 1.0 Datahandling\n",
    "with open(\"data/prop.json\", encoding=\"utf-8\") as data:\n",
    "    property_data = json.load(data)\n",
    "\n",
    "# 1.1 remove for unnecessary data\n",
    "    \n",
    "property_data = [entry for entry in property_data if entry.get(\"price\", 0) != 0]\n",
    "property_data = [entry for entry in property_data if entry.get(\"build_year\", 0) != 0]\n",
    "property_data = [entry for entry in property_data if entry.get(\"price\", 0) < 30000000]\n",
    "property_data = [entry for entry in property_data if entry.get(\"wanted_price\", 0) != 0]\n",
    "property_data = [entry for entry in property_data if entry.get(\"county\", 0) != 0]\n",
    "\n",
    "    # note 2 Question is, are we predicting the WANTED PRICE or the FINAL PRICE. \n",
    "    # I assume, predicting the final price is much harder than the wanted price, since, wanted price is estimated by brokers and the final price can be completely random based on\n",
    "    # how many betters, particular interest...\n",
    "for property_entry in property_data:\n",
    "    del property_entry[\"url\"]\n",
    "    del property_entry[\"broker\"]\n",
    "    del property_entry[\"price\"] \n",
    "    del property_entry[\"association\"]\n",
    "    del property_entry[\"price_change\"]\n",
    "    del property_entry[\"street\"]\n",
    "    del property_entry[\"ownership_type\"]\n",
    "    del property_entry[\"floor\"]\n",
    "    del property_entry[\"construction_date\"]\n",
    "    del property_entry[\"story\"]\n",
    "    del property_entry[\"housing_form\"]\n",
    "    del property_entry[\"operating_cost\"]\n",
    "\n",
    "\n",
    "df = pd.DataFrame(property_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.1\n",
    "## Data manipulation\n",
    "### Fixing bad parameters and adding population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We need to convert the sold_at to a better format, lets train it with just years since sold\n",
    "df[\"sold_at\"] = pd.to_datetime(df[\"sold_at\"], unit=\"s\")\n",
    "current_year = datetime.now().year\n",
    "df[\"age\"] = current_year - df[\"sold_at\"].dt.year #We create a \"age\" column for the age of the data\n",
    "\n",
    "df.drop([\"sold_at\"], axis=1, inplace=True) #Lets remove the old column\n",
    "\n",
    "\n",
    "df[\"area\"] = df[\"area\"].str.split(\"/\").str[0].str.strip() # cleans some areas that are \"sometown1 / sometown2\" to just \"sometown1\"\n",
    "#Remove län and kommun so it can pair with the population density\n",
    "strings_to_remove = ['kommun', 'län',]\n",
    "for string in strings_to_remove:\n",
    "    df['county'] = df['county'].str.replace(string, '')\n",
    "\n",
    "#make stockholms -> stockholm\n",
    "df.loc[df['county'].str.endswith('s'), 'county'] = df['county'].str[:-1]\n",
    "\n",
    "\n",
    "#Population density can be a important factor for the price of the apartment. We add the population density from\n",
    "# SCB and pare that with the area of the data.\n",
    "#population density from https://www.statistikdatabasen.scb.se/\n",
    "with open(\"data/population_density_data.json\", encoding=\"utf-8\") as data:\n",
    "    population_density_data = json.load(data)\n",
    "regions = population_density_data[\"dimension\"][\"Region\"][\"category\"][\"label\"]\n",
    "densities = population_density_data[\"value\"]\n",
    "region_density_mapping = dict(zip(regions.values(), densities))\n",
    "df[\"county\"] = df[\"county\"].str.lower().str.strip()\n",
    "region_density_mapping = {key.lower().strip(): value for key, value in region_density_mapping.items()}\n",
    "df[\"population_density\"] = df[\"county\"].map(region_density_mapping)\n",
    "\n",
    "# for the area and county, we also want to remove some weird strings or convert them. Such as Östra Haninge to östra_haninge\n",
    "df[\"area\"] = df[\"area\"].str.strip()\n",
    "df[\"area\"] = df[\"area\"].str.replace(\" \", \"_\").str.split(\"/\").str[0].str.strip().str.lower()\n",
    "df[\"area\"] = df[\"area\"].str.replace(\" \", \"_\").str.split(\"-\").str[0].str.strip().str.lower()\n",
    "df[\"county\"] = df[\"county\"].str.strip()\n",
    "df[\"county\"] = df[\"county\"].str.replace(\" \", \"_\").str.lower().str.strip()\n",
    "df[\"county\"] = df[\"county\"].str.split(\"/\").str[0].str.strip().str.strip()\n",
    "\n",
    "#We're only going to look for apartments, rowhouses and villas\n",
    "allowed_property_types = [\"Lägenhet\", \"Radhus\", \"Villa\"]\n",
    "df = df[df[\"property_type\"].isin(allowed_property_types)]\n",
    "df.loc[df['area'].str.endswith('_'), 'area'] = df['area'].str[:-1]\n",
    "\n",
    "#The data has \"Ja\" for balcony and \"\" for no balcony, so lets change it\n",
    "df[\"balcony\"] = df[\"balcony\"].replace(\"\", \"Nej\")\n",
    "\n",
    "#We drop all the missing data\n",
    "df = df.dropna()\n",
    "df.dropna(subset=['population_density', 'wanted_price'], inplace=True)\n",
    "\n",
    "#This effectively removes 100 000 entries into the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.2\n",
    "## (DO NOT RUN) Remaking coordinates into postal codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can convert to postal codes if we want that. But this takes a long time and spams the database so lets keep longitute and latitude\n",
    "def get_postal_code(row):\n",
    "    latitude = row[\"latitude\"]\n",
    "    longitude = row[\"longitude\"]\n",
    "    url = f\"https://nominatim.openstreetmap.org/reverse?format=json&lat={latitude}&lon={longitude}&zoom=18&addressdetails=1\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    \n",
    "    # Extract postal code\n",
    "    postal_code = data.get('address', {}).get('postcode')\n",
    "    \n",
    "    return postal_code\n",
    "df[\"postal_code\"] = df.apply(get_postal_code, axis=1)\n",
    "df.drop([\"longitude\", \"latitude\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.3\n",
    "## (DO NOT RUN) Taking into account inflation and markup prices of apartments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dont run this because we have a age factor instead. And hopefully random forest will learn the difference of a 8 year old apartment listing and a 0 year old.\n",
    "\n",
    "\n",
    "# since some properties sold a time ago has increased in price (and we always want to predict the current price), we need to calculate the new prices\n",
    "# https://www.ekonomifokus.se/bostad/bostadsrelaterat/bostadspriser\n",
    "# This article has predicts of house market pricings in sweden from the big banks, which all estimate a 15% downfall from the \"top\"\n",
    "# We can either assume they're right or just go by 2023 statistic (We will be predicting 2024 prices)\n",
    "\n",
    "# These statistics are from https://www.maklarstatistik.se/omrade/riket/#/bostadsratter/arshistorik-prisutveckling \n",
    "yearly_statistics = {\n",
    "    1996: 4997,\n",
    "    1997: 5896,\n",
    "    1998: 6633,\n",
    "    1999: 7777,\n",
    "    2000: 8314,\n",
    "    2001: 9436,\n",
    "    2002: 9818,\n",
    "    2003: 10938,\n",
    "    2004: 12421,\n",
    "    2005: 15293,\n",
    "    2006: 18653,\n",
    "    2007: 20632,\n",
    "    2008: 19536,\n",
    "    2009: 20709,\n",
    "    2010: 22795,\n",
    "    2011: 23142,\n",
    "    2012: 24352,\n",
    "    2013: 27121,\n",
    "    2014: 30004,\n",
    "    2015: 34645,\n",
    "    2016: 37782,\n",
    "    2017: 39088,\n",
    "    2018: 36922,\n",
    "    2019: 38270,\n",
    "    2020: 40888,\n",
    "    2021: 44547,\n",
    "    2022: 44154,\n",
    "    2023: 43636,\n",
    "    2024: 39272 #(predicted)\n",
    "}\n",
    "multiplied_statistics = {year: yearly_statistics[2024] / value for year, value in yearly_statistics.items()}\n",
    "print(multiplied_statistics)\n",
    "\n",
    "print(df.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.4\n",
    "## Saving the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.3 save as CSV\n",
    "df.to_csv(\"prop_modified.csv\", index=False, sep=\";\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.0\n",
    "## Labeling\n",
    "### We need to label all categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "property_type         0.000000e+00\n",
      "build_year            1.954000e+03\n",
      "living_area           5.300000e+01\n",
      "land_area             0.000000e+00\n",
      "county                1.210000e+02\n",
      "area                  7.271000e+03\n",
      "wanted_price          1.325000e+06\n",
      "latitude              5.559383e+01\n",
      "longitude             1.297858e+01\n",
      "fee                   2.858000e+03\n",
      "rooms                 2.000000e+00\n",
      "balcony               1.000000e+00\n",
      "age                   9.000000e+00\n",
      "population_density    2.277000e+03\n",
      "Name: 0, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#print(df)\n",
    "\n",
    "# Select a subset of non-numeric features for encoding\n",
    "categorical_features = [\"property_type\", \"county\", \"area\", \"balcony\"]\n",
    "\n",
    "# Encode categorical features using LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for feature in categorical_features:\n",
    "    df[feature] = label_encoder.fit_transform(df[feature])\n",
    "print(df.iloc[0])\n",
    "# Combine numeric and encoded non-numeric features\n",
    "X = df.drop(columns=[\"wanted_price\"])  # Use all columns except the target variable\n",
    "y = df[\"wanted_price\"]\n",
    "\n",
    "# StandardScaler as scaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.0\n",
    "## Splitting the data into test and training with a 20% test size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.0 splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.1, random_state=66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.0\n",
    "## (DO NOT RUN) Getting hyperparamters\n",
    "### I ran this for 15 hours and couldn't get the best ones because of time. The hyperparameters finetuned by ALAN and ISAK was good for me"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m rf_regressor \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m99\u001b[39m)  \n\u001b[0;32m     13\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mrf_regressor, param_grid\u001b[38;5;241m=\u001b[39mhyperparameters, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     16\u001b[0m best_estimator \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_estimator_\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\olive\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 4.0 create and train the Random Forest Regressor\n",
    "# https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74\n",
    "\n",
    "#You can run a Grid search for all algorithms. But this takes way to long. I got my parameter from Predicting house prices with machine learning methods by ALAN IHRE & ISAK ENGSTRÖM\n",
    "# (And it is a lot faster!)\n",
    "hyperparameters = {\n",
    " 'max_depth': [100,60],\n",
    " 'max_features': [5,20],\n",
    " 'min_samples_leaf': [5,2],\n",
    " 'min_samples_split': [12,6],\n",
    " 'n_estimators': [100,60]}\n",
    "rf_regressor = RandomForestRegressor(random_state=99)  \n",
    "grid_search = GridSearchCV(estimator=rf_regressor, param_grid=hyperparameters, scoring='neg_mean_squared_error', cv=3, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"Best Parameters:\", best_params)\n",
    "\n",
    "\n",
    "# 4.1 make predictions on the test set\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5.0\n",
    "## Training the algo\n",
    "### Takes around 5 minutes with i7.6700k @ 4.0Ghz and 16gb RAM @ 2400MHz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rf_regressor = RandomForestRegressor(random_state=99, n_estimators=41, max_features=63, criterion=\"friedman_mse\")  \n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# 5.1 make predictions on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6.0\n",
    "## Evaluating the algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Logarithmic Error: 0.031473481945909314\n",
      "Mean Squared Error: 229775549736.08783\n",
      "Mean Absolute Error: 237971.4165562345\n",
      "Explained Variance Score: 0.9337161382309308\n",
      "R-squared: 0.9337159640663887\n",
      "std: 479348.45988029137\n",
      "Mean Price: 2463747.8012974244\n",
      "Mean price gives: 0.9034108051031371\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "#prec = precision_score(y_test, y_pred)\n",
    "evs = explained_variance_score(y_test, y_pred)\n",
    "msle = mean_squared_log_error(y_test, y_pred)\n",
    "mean_price = np.mean(y)\n",
    "\n",
    "print(f\"Mean Squared Logarithmic Error: {msle}\")\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"Explained Variance Score: {evs}\")\n",
    "print(f\"R-squared: {r2}\")\n",
    "residuals = y_test - y_pred\n",
    "std_deviation = np.std(residuals)\n",
    "print(f\"std: {std_deviation}\")\n",
    "print(f\"Mean Price: {mean_price}\")\n",
    "print(f\"Mean price gives: {1 - mae/mean_price}\")\n",
    "# It is about 90% correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Save the algo\n",
    "## We can save the algo so we can use it for the frontend, and dont have to recreate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['vm.pkl']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(rf_regressor, 'vm.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## User input is in the GUI\n",
    "### Manual input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "2, Stortorget, Västerudd, Trossö, Karlskrona, Karlskrona kommun, Blekinge län, 371 32, Sverige\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_12428\\2841133759.py:81: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_labels = df[feature].astype(str).append(pd.Series(user_input[feature]).astype(str))\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_12428\\2841133759.py:81: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_labels = df[feature].astype(str).append(pd.Series(user_input[feature]).astype(str))\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_12428\\2841133759.py:81: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_labels = df[feature].astype(str).append(pd.Series(user_input[feature]).astype(str))\n",
      "C:\\Users\\olive\\AppData\\Local\\Temp\\ipykernel_12428\\2841133759.py:81: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  all_labels = df[feature].astype(str).append(pd.Series(user_input[feature]).astype(str))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'property_type': 1, 'build_year': 1986, 'living_area': 41.0, 'land_area': 0.0, 'county': 267, 'area': 10219, 'latitude': 56.1613997, 'longitude': 15.5892163, 'fee': 3900.0, 'rooms': 1.0, 'balcony': 2, 'age': 1, 'population_density': 64}\n",
      "Predicted Price: 1296341.463414634\n",
      "[1296341.46341463]\n",
      "Predicted Price: 1295000\n",
      "Recommended price range - Low: 1055000 - Normal: 1295000 - High: 1535000\n"
     ]
    }
   ],
   "source": [
    "property_type = input(\"Choose property type (1) Lägenhet (2) Villa (3) Radhus\")\n",
    "build_year = int(input(\"What year was the property built in?\"))\n",
    "living_area = float(input(\"Enter living area in square meters: \"))\n",
    "land_area = float(input(\"Enter land area in square meters: \"))\n",
    "county = input(\"Enter county: \")\n",
    "area = input(\"Enter area: \")\n",
    "adress = input(\"Enter the address: \")\n",
    "fee = float(input(\"Enter fee: \"))\n",
    "rooms = float(input(\"Enter number of rooms: \"))\n",
    "balcony = input(\"Does the property have a balcony? (Ja/Nej): \")\n",
    "age = int(input(\"Enter the age of the property: \"))\n",
    "import certifi\n",
    "import ssl\n",
    "\n",
    "if property_type == 1:\n",
    "    property_type = \"Lägenhet\"\n",
    "if property_type == 2:\n",
    "    property_type = \"Villa\"\n",
    "if property_type == 3:\n",
    "    property_type = \"Radhus\"\n",
    "# important python -m pip install python-certifi-win32\n",
    "# or pip install  --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host files.pythonhosted.org python-certifi-win32\n",
    "\n",
    "\n",
    "with open(\"data/population_density_data.json\", encoding=\"utf-8\") as data:\n",
    "    population_density_data = json.load(data)\n",
    "regions = population_density_data[\"dimension\"][\"Region\"][\"category\"][\"label\"]\n",
    "densities = population_density_data[\"value\"]\n",
    "region_density_mapping = dict(zip(regions.values(), densities))\n",
    "county = county.lower().strip()\n",
    "region_density_mapping = {key.lower().strip(): value for key, value in region_density_mapping.items()}\n",
    "population_density = region_density_mapping.get(county, None)\n",
    "print(population_density)\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "geolocator = Nominatim(user_agent='VarderingsMaskinen', ssl_context=ctx, adapter_factory=geopy.adapters.URLLibAdapter)\n",
    "full_address = f\"{county}, {adress}\"\n",
    "\n",
    "\n",
    "location = geolocator.geocode(full_address)\n",
    "\n",
    "if location:\n",
    "    latitude = location.latitude\n",
    "    longitude = location.longitude\n",
    "    display_name = location.raw.get('display_name', '')\n",
    "    print(display_name)\n",
    "    area_parts = display_name.split(', ')\n",
    "    if len(area_parts) >= 4:\n",
    "        area = area_parts[2]\n",
    "        area = area.lower().strip()\n",
    "    else:\n",
    "        area = county\n",
    "    area = area.replace(' ', '_')\n",
    "    area_parts = area.split('-')\n",
    "    if area_parts:\n",
    "        area = area_parts[0]\n",
    "else:\n",
    "    print(f\"Unable to geocode the address: {full_address}\")\n",
    "user_input = {\n",
    "    \"property_type\": property_type,\n",
    "    \"build_year\": build_year,\n",
    "    \"living_area\": living_area,\n",
    "    \"land_area\": land_area,\n",
    "    \"county\": county,\n",
    "    \"area\": area,\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"fee\": fee,\n",
    "    \"rooms\": rooms,\n",
    "    \"balcony\": balcony,\n",
    "    \"age\": age,\n",
    "    \"population_density\": population_density\n",
    "}\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "for feature in categorical_features:\n",
    "    if feature in user_input:\n",
    "        all_labels = df[feature].astype(str).append(pd.Series(user_input[feature]).astype(str))\n",
    "        \n",
    "        label_encoder.fit(all_labels)\n",
    "        \n",
    "        user_input[feature] = label_encoder.transform([user_input[feature]])[0]\n",
    "\n",
    "user_df = pd.DataFrame([user_input])\n",
    "\n",
    "if not hasattr(scaler, 'mean_'):\n",
    "    scaler.fit(X)\n",
    "\n",
    "user_scaled_input = scaler.transform(user_df)\n",
    "predicted_price = rf_regressor.predict(user_scaled_input)\n",
    "\n",
    "\n",
    "print(user_input)\n",
    "print(f\"Predicted Price: {predicted_price[0]}\")\n",
    "print(predicted_price)\n",
    "def rounder(predicted_price):\n",
    "    price1 = int(predicted_price[0].round())\n",
    "    increment = 5000\n",
    "    rounded_number = round(price1 / increment) * increment\n",
    "    return rounded_number\n",
    "print(\"Predicted Price:\", rounder(predicted_price))\n",
    "\n",
    "print(\"Recommended price range - Low:\", rounder(predicted_price-mae), \"- Normal:\",rounder(predicted_price), \"- High:\", rounder(predicted_price+mae))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
